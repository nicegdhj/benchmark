import os
import os.path as osp
import re
import subprocess
import multiprocessing
import sys
import time
import torch
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from threading import Lock
from typing import Any, Dict, List, Tuple

import mmengine
import numpy as np
from mmengine.config import ConfigDict
from mmengine.device import is_npu_available

from ais_bench.benchmark.registry import RUNNERS, TASKS
from ais_bench.benchmark.utils.core.abbr import task_abbr_from_cfg
from ais_bench.benchmark.runners.base import TasksMonitor
from ais_bench.benchmark.runners.base import BaseRunner
from ais_bench.benchmark.utils.logging.error_codes import RUNNER_CODES
from ais_bench.benchmark.utils.logging.exceptions import ParameterValueError


def get_command_template(gpu_ids: List[int]) -> str:
    """Format command template given available gpu ids."""
    if is_npu_available() and len(gpu_ids) > 0:
        tmpl = 'ASCEND_RT_VISIBLE_DEVICES=' + ','.join(str(i) for i in gpu_ids)
        tmpl += ' {task_cmd}'
    elif sys.platform == 'win32':  # Always return win32 for Windows
        # use command in Windows format
        tmpl = 'set CUDA_VISIBLE_DEVICES=' + ','.join(str(i) for i in gpu_ids)
        tmpl += ' & {task_cmd}'
    else:
        tmpl = 'CUDA_VISIBLE_DEVICES=' + ','.join(str(i) for i in gpu_ids)
        tmpl += ' {task_cmd}'
    return tmpl


def monitor_process(task_names, output_path, is_debug, refresh_interval=0.5):
    tasks_monitor = TasksMonitor(task_names, output_path, is_debug, refresh_interval)
    tasks_monitor.launch_state_board()


@RUNNERS.register_module()
class LocalRunner(BaseRunner):
    """Local runner. Start tasks by local python.

    Args:
        task (ConfigDict): Task type config.
        max_num_workers (int): Max number of workers to run in parallel.
            Defaults to 16.
        max_workers_per_gpu (int): Max number of workers to run for one GPU.
            Defaults to 1.
        debug (bool): Whether to run in debug mode.
    """

    def __init__(self,
                 task: ConfigDict,
                 max_num_workers: int = 16,
                 debug: bool = False,
                 max_workers_per_gpu: int = 1,
                 keep_tmp_file: bool = False,
                 **kwargs):
        super().__init__(task=task, debug=debug)
        self.max_num_workers = max_num_workers
        self.max_workers_per_gpu = max_workers_per_gpu
        self.keep_tmp_file = keep_tmp_file
        for k, v in kwargs.items():
            self.logger.warning(f'Ignored argument in {self.__module__}: {k}={v}')

    def launch(self, tasks: List[Dict[str, Any]]) -> List[Tuple[str, int]]:
        """Launch multiple tasks.

        Args:
            tasks (list[dict]): A list of task configs, usually generated by
                Partitioner.

        Returns:
            list[tuple[str, int]]: A list of (task name, exit code).
        """
        self.logger.debug(f"LocalRunner.launch called with {len(tasks)} task(s)")
        task_names = [task_abbr_from_cfg(task) for task in tasks]

        monitor_p = multiprocessing.Process(
            target=monitor_process,
            args=(task_names, tasks[0]['work_dir'], self.debug, 0.5)

        )
        self.logger.debug(f"Task monitor process started (PID: {monitor_p.pid})")

        if is_npu_available():
            visible_devices = 'ASCEND_RT_VISIBLE_DEVICES'
            device_nums = torch.npu.device_count()
        else:
            visible_devices = 'CUDA_VISIBLE_DEVICES'
            device_nums = torch.cuda.device_count()
        if visible_devices in os.environ:
            all_gpu_ids = [
                int(i)
                for i in re.findall(r'(?<!-)\d+', os.getenv(visible_devices))
            ]
        else:
            all_gpu_ids = list(range(device_nums))

        self.logger.debug(f"Available devices: {all_gpu_ids} (total: {device_nums}, type: {visible_devices})")
        monitor_p.start()

        if self.debug:
            status = self._run_debug(tasks, all_gpu_ids, monitor_p)
        else:
            status = self._run_normal(tasks, all_gpu_ids, monitor_p)
        monitor_p.join()
        TasksMonitor.rm_tmp_files(tasks[0]['work_dir'])
        self.logger.debug(f"LocalRunner.launch completed, {len(status)} task(s) finished")
        return status

    def _run_debug(self, tasks: List[Dict[str, Any]], all_gpu_ids: List[int], monitor_p: multiprocessing.Process):
        """Launch multiple tasks.

        Args:
            tasks (list[dict]): A list of task configs, usually generated by
                Partitioner.

        Returns:
            list[tuple[str, int]]: A list of (task name, exit code).
        """
        status = []
        for task in tasks:
            task = TASKS.build(dict(cfg=task, type=self.task_cfg['type']))
            task_name = task.name
            num_gpus = task.num_gpus if hasattr(task, 'num_gpus') else 0
            self.logger.debug(f"Debug mode: launching task '{task_name}' with {num_gpus} GPU(s)")
            assert len(all_gpu_ids) >= num_gpus
            # get cmd
            mmengine.mkdir_or_exist('tmp/')
            import uuid
            uuid_str = str(uuid.uuid4())

            param_file = f'tmp/{uuid_str}_params.py'
            try:
                task.cfg.dump(param_file)
                # if use torchrun, restrict it behaves the same as non
                # debug mode, otherwise, the torchrun will use all the
                # available resources which might cause inconsistent
                # behavior.
                if len(all_gpu_ids) > num_gpus and num_gpus > 0:
                    self.logger.warning(f'Only use {num_gpus} GPUs for '
                                            f'total {len(all_gpu_ids)} '
                                            'available GPUs in debug mode.')
                tmpl = get_command_template(all_gpu_ids[:num_gpus])
                cmd = task.get_command(cfg_path=param_file, template=tmpl)

                proc = subprocess.Popen(cmd, shell=True, text=True)
                try:
                    proc.wait()
                except KeyboardInterrupt:
                    monitor_p.join()
                    self.logger.warning(f"Subprocess of task:{task_name} interrupted by user!")
                    proc.wait()  # ensure subprocess finished
            finally:
                if not self.keep_tmp_file:
                    os.remove(param_file)
                else:
                    pass
            status.append((task_name, 0))
        return status

    def _run_normal(self, tasks: List[Dict[str, Any]], all_gpu_ids: List[int], monitor_p: multiprocessing.Process):
        """Launch multiple tasks.

        Args:
            tasks (list[dict]): A list of task configs, usually generated by
                Partitioner.

        Returns:
            list[tuple[str, int]]: A list of (task name, exit code).
        """
        status = []
        if len(all_gpu_ids) > 0:
            gpus = np.zeros(max(all_gpu_ids) + 1, dtype=np.uint)
            gpus[all_gpu_ids] = self.max_workers_per_gpu
            self.logger.debug(f"GPU resource pool initialized: {len(all_gpu_ids)} GPUs with {self.max_workers_per_gpu} workers per GPU")
        else:
            gpus = np.array([], dtype=np.uint)
            self.logger.debug("No GPU devices available, running on CPU")

        lock = Lock()

        def submit(task, index):
            task = TASKS.build(dict(cfg=task, type=self.task_cfg['type']))
            num_gpus = task.num_gpus if hasattr(task, 'num_gpus') else 0
            if len(gpus) < num_gpus:
                raise ParameterValueError(RUNNER_CODES.NOT_ENOUGH_GPUS, f"Not enough GPUs available, only {len(gpus)} GPUs found, but {num_gpus} GPUs are required for task {task.name}")

            while True:
                lock.acquire()
                if sum(gpus > 0) >= num_gpus:
                    gpu_ids = np.where(gpus)[0][:num_gpus]
                    gpus[gpu_ids] -= 1
                    lock.release()
                    break
                lock.release()
                time.sleep(1)

            res = self._launch(task, gpu_ids, index)

            with lock:
                gpus[gpu_ids] += 1

            return res

        with ThreadPoolExecutor(
                max_workers=self.max_num_workers) as executor:
            try:
                self.logger.debug(f"ThreadPoolExecutor started with {self.max_num_workers} max workers")
                status = list(executor.map(submit, tasks, range(len(tasks))))
            except KeyboardInterrupt:
                monitor_p.join()
                self.logger.warning("Main process interrupted by user! Waiting for running tasks to complete...")
                status = list(status) if status is not None else []

        return status

    def _launch(self, task, gpu_ids, index):
        """Launch a single task.

        Args:
            task (BaseTask): Task to launch.

        Returns:
            tuple[str, int]: Task name and exit code.
        """

        task_name = task.name

        pwd = os.getcwd()
        # Dump task config to file
        mmengine.mkdir_or_exist('tmp/')
        # Using uuid to avoid filename conflict
        import uuid
        uuid_str = str(uuid.uuid4())
        param_file = f'{pwd}/tmp/{uuid_str}_params.py'

        try:
            task.cfg.dump(param_file)
            tmpl = get_command_template(gpu_ids)
            get_cmd = partial(task.get_command,
                              cfg_path=param_file,
                              template=tmpl)
            cmd = get_cmd()

            # Run command
            out_path = task.get_log_path(file_extension='out')
            mmengine.mkdir_or_exist(osp.split(out_path)[0])
            with open(out_path, 'w', encoding='utf-8') as stdout:
                result = subprocess.run(cmd,
                                        shell=True,
                                        text=True,
                                        stdout=stdout,
                                        stderr=stdout)
            if result.returncode != 0:
                self.logger.error(RUNNER_CODES.TASK_FAILED, f"{task_name} failed with code {result.returncode}, see\n{out_path}")
        finally:
            # Clean up
            if not self.keep_tmp_file:
                os.remove(param_file)
            else:
                pass
        return task_name, result.returncode
